{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math+Econ+Code Prerequisites 2: Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* A first course in Optimization theory (Sundaram, R.), 1996\n",
    "* Further Mathematics for Economic Analysis 2nd Ed (Sydsaeter et al), 2008\n",
    "* Dynamic Programming and Optimal Control 4th Ed (Bertsekas), 2018 (Advanced)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and motivation\n",
    "\n",
    "A dynamic programming problem is an optimization problem in which decisions are taken sequentially over several time periods (Sundaram).\n",
    "\n",
    "Typically, a dynamic optimization problem arises when actions today have consequences tomorrow. You have probably solved dynamic programming problems already, at least in problems with a finite horizon.\n",
    "\n",
    "The specificity of DP problems is that they are set up in discrete time. Time is index with $t = 1,2,3...,T/\\infty$. Two main subtypes of DP problems exist:\n",
    "* Finite-Horizon Dynamic Programming (FHDP), in which time stops at $T < \\infty$\n",
    "* Infinite-Horizon/Stationary Dynamic Programming (SDP), in which the agents plans as if he was immortal.\n",
    "\n",
    "Many interesting problems (such as matching), have a dynamic counterpart and the analysis of these problems is technically and intellectually challenging. Under the form of dynamic discrete choice, dynamic programming also has applications in econometrics. In any case, you should have a good understanding of dynamic programming to tackle any graduate macroeconomics class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* FHDP: general setup of the problem\n",
    "* How to solve an FHDP\n",
    "* SDP: general setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite Horizon DP\n",
    "#### The general setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the general form of a FHDP problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\max_{\\{u_t\\}_{t=0}^T \\in U}  & \\, \\sum_{t=0}^T f(x_t, u_t) \\\\\n",
    "s.t: & \\ x_{t+1} = g(x_t, u_t) \\\\\n",
    "& \\ x_0 \\hspace{5pt} \\text{given}\n",
    "\\end{align}\n",
    "\n",
    "$x_t$ is the **state** variable. A key rule of DP is that the state should contain all the information available and necessary for the agent to make the optimal decision. In the simple problems we will tackle here, $x_t$ will be a real number. $x_0$ is given: it is the inital state.\n",
    "\n",
    "$u_t$ is the decision, called the **control** of the agent. It belongs to a set $U$. For example, in the consumption problem we are about to describe, $u$ represents consumption, and it cannot be higher than the total amount of resource available (if we exclude credit of course).\n",
    "\n",
    "$g$ is the transition function: taking the state $x_t$ at $t$ and the decision $u_t$ at $t$ (and time) as input, it outputs the new state.\n",
    "\n",
    "$f$ is the objective function. In economics, it is typically some measure of utility, profit, or welfare in general. It depends on time, the state, and the decision taken by the agent.\n",
    "\n",
    "#### Example: dynamic consumption\n",
    "This is the \"Hello World\" of Dynamic Programming for economists.\n",
    "$x_t$ is the agent's wealth at time $t$. The control $u_t$ is the proportion of wealth that the agent consumes: $c_t = u_t x_t$. The remaining $(1 - u_t) x_t$ are invested and come back with an interest rate at $x_{t+1} = \\rho (1 - u_t) x_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to solve a FHDP problem.\n",
    "\n",
    "Given the assumption that $x_t$ contains all necessary information, it should be possible, at any $t$, to define by $V(x_t)$ the **value** of being at $x$, at $t$. Let's suppose that tomorrow, at $t=s+1$, we know what the optimal control $u_t^*$ should be for $t = s+1, s+2,...,T$. Then, we could write:\n",
    "\\begin{align}\n",
    "V(x_{s+1}) = \\sum_{t=s+1}^T f(x_t, u_t^*)\n",
    "\\end{align}\n",
    "\n",
    "Now, let's go back one day: today, at $t=s$. If the agent knows $V(x_{s+1})$ and the transition function $x_{s+1} = g(x_{s}, u_{s})$, the only thing the agent has to do is to find the control $u_{s}$ that maximizes\n",
    "$$f(x_{s}, u_{s}) + V(g(x_{s}, u_{s}))$$\n",
    "\n",
    "Finding this optimal control **today** is a static problem it only depends on present variables: the agent tries to find the control that maximizes his welfare today and brings him to the most favourable state **tomorrow** (the one where the value function V is highest).\n",
    "\n",
    "Then, today if the agent knows $V(x_{s+1})$ and $\\{u_t^*\\}_{s+1}^T$, he can determine $u_s^*$. Then, he can compute $V(x_s)$, which allows him to go still one day earlier, determine the optimal control then, and so on. This is the fundamental equation of Dynamic Programming, that allows us to write sequentially what would otherwise be a diffult problem:\n",
    "\n",
    "\\begin{align}\n",
    "V(x_t) = max_{u_t \\in U} \\Big[ f(x_t, u_t) + V( x_{t+1} ) \\Big]\n",
    "\\end{align}\n",
    "\n",
    "To solve this type of problem, you need to start from the last point in time: at $t=T$ there is no tomorrow, so the agent just optimizes for today: $V(x_T) = max_{u_T \\in U} f(x_T, u_T)$. Then, you go one day before and solve for $u_{T-1}^*$ by optimizing as shown. This process of moving backwards in time to solve a sequential problem is called **backwards induction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: solve the following FHDP problem:\n",
    "        \n",
    "\\begin{align}\n",
    "max_u \\sum_{t=0}^2 (1 + x_t - u_t^2), \\\\\n",
    "x_{t+1} &= x_t + u_t, \\\\\n",
    "x_0 &= 0, \\\\\n",
    "u_t &\\in \\mathbb{R}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$t=2: \\hspace{10pt} V(x_T) = \\max_{u_2} 1 + x_2 - u_2^2$\n",
    "\n",
    "Clearly, $u_2^* = 0$. So, $V(x_T) = 1 + x_2^*$\n",
    "\n",
    "$t=1: \\hspace{10pt} V(x_1) = \\max_{u_1} 1 + x_1 - u_1^2 + V(x_T)$\n",
    "\n",
    "$ \\hspace{60pt} = \\max_{u_1} 1 + x_1 - u_1^2 + 1 + x_2$\n",
    "\n",
    "$ \\hspace{60pt} = \\max_{u_1} 1 + x_1 - u_1^2 + 1 + x_1 + u_1$\n",
    "\n",
    "FOC: $-2u_1 + 1 = 0 \\Leftrightarrow u_1^* = \\frac{1}{2}$ (Concavity is trivial)\n",
    "\n",
    "$\\Rightarrow V(x_1) = 1 + x_1^* - \\frac{1}{4} + 1 + x_1^* + \\frac{1}{2} = 2 x_1^* + \\frac{9}{4}$\n",
    "\n",
    "\n",
    "$t=0: \\hspace{10pt} V(x_0) = \\max_{u_0} 1 + x_0 - u_0^2 + 2x_1^* + \\frac{9}{4}$\n",
    "\n",
    "$ \\hspace{60pt} =  \\max_{u_0} 1 + x_0 - u_0^2 + 2(x_0 + u_0) + \\frac{9}{4}$\n",
    "\n",
    "FOC: $-2u_0 + 2 = 0 \\Leftrightarrow u_0^* = 1$ (Concavity is trivial again)\n",
    "\n",
    "So, given the initial state $x_0=0$, the optimal sequence of actions and states is the following:\n",
    "* $t=0$: $x_0=0$, $u_0^* = 1$\n",
    "* $t=1$: $x_1=1$, $u_1^* = \\frac{1}{2}$\n",
    "* $t=2$: $x_2=\\frac{3}{2}$, $u_2^* = \\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word on Stationary Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceiving dynamic programming at an infinite horizon requires a bit of a conceptual jump from finite horizon dynamic programming.\n",
    "\n",
    "First, why is it called \"stationary\" ? If an agent lives forever, he is free from a strategic use of the time, and therefore he will find an optimal consumption level, and a consumption rule that allows him to reach that optimal consumption level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General setup\n",
    "\\begin{align}\n",
    "\\max_{\\{u_t\\}_{t=0}^T \\in U}  & \\, \\sum_{t=0}^{\\infty} \\beta^t f(x_t, u_t) \\\\\n",
    "s.t: & \\ x_{t+1} = g(x_t, u_t) \\\\\n",
    "& \\ x_0 \\hspace{5pt} \\text{given}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This is very close to the finite setting. The modifications is that now, the agent optimizes an infinite sum. To avoid that this infinite sum diverges to $+\\infty$, the future flows of utility are discounted with $\\beta \\in (0,1)$. This way, at moment $t$, if $\\beta=0.8$, utility today is weighted $\\beta^0 = 1$, utility tomorrow is weighted $\\beta^1 = 0.8$, after tomorrow $\\beta^2 = 0.64$, etc. The higher $\\beta$, the more **patient** the agent is.\n",
    "\n",
    "Since the behaviour is stationary, the goal of SDP is to find a fixed **policy function** $h$, a function that takes the state as input, and returns a value for the control. The goal is that, once this policy function is known, the whole system can be simulated with the policy function and the transition function: $u_t = h(x_t)$ and $x_{t+1} = g(x_t, u_t)$, starting from any $x_0$ given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewriting the SDP problem as a sequential problem:\n",
    "\n",
    "Let us suppose that we are at time $t = 0$. The problem that the agent has to solve is the following:\n",
    "\n",
    "\\begin{align}\n",
    "V(x_0) = \\max & \\, \\sum_{t=0}^{\\infty} \\beta^t f(x_t, u_t) \\\\\n",
    "= \\max & \\, f(x_0, u_0) + \\sum_{t=1}^\\infty \\beta^t f(x_t, u_t) \\\\\n",
    "= \\max & \\, f(x_0, u_0) + \\beta \\sum_{t=1}^\\infty \\beta^{t-1} f(x_t, u_t) \\\\\n",
    "= \\max & \\, f(x_0, u_0) + \\beta V(x_1)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Since the problem is stationary, the time indexes do not matter: we can denote $x$ for any $x_t$ and $x'$ for any $x_{t+1}$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "V(x) = \\max f(x, u) + \\beta V(x')\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving a SDP problem\n",
    "\n",
    "It is harder to do with pen and paper than FHDP. In general, solving such a problem requires a fixed point algorithm called \"Value Function Iteration\" (VFI). The algorithm can be simply described as follows:\n",
    "\n",
    "Before the iteration, fix a starting function V, a maximum number of iteration and a tolerance\n",
    "\n",
    "* Fix a random function $V_{init}$, applied to $x'$\n",
    "* Maximize for the optimal control at $x$\n",
    "* Construct the new function $V_{new}$\n",
    "\n",
    "if $V_{new} - V_{init} < tolerance$  $\\forall x$: stop the iteration\n",
    "\n",
    "else: fix $V_{init} = V_{new}$\n",
    "\n",
    "back to step 1\n",
    "    \n",
    "This is, of course, highly stylized, but you will not need VFI for the masterclass. However it is a core tool for Graduate Macro.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Programming also has a stochastic extension, in which the state transition involves probabilities. In such settings, the number of possible states can very quickly explode and become intractable: that's called the **curse of dimensionality**. Algorithms from the Artificial Intelligence literature are strongly connected to the ones we presented here (think of the position of pieces on the chess board as being the state, and the control as being the move that can be made by the chess player).\n",
    "\n",
    "If you are interested in these topics, you are invited to take a look at Dimitri Bertsekas's website, and especially his [work on Dynamic Programming](http://www.athenasc.com/dpbook.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
