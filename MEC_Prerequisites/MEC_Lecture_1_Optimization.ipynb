{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math+Econ+Code Prerequisites 1: Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "* Mathematics for Economists (Simon Blume), 1994 (Introductive)\n",
    "* A First Course in Optimization Theory (Sundaram, R.), 1996\n",
    "* Further Mathematics for Economic Analysis (Sydsaeter et al), 2008\n",
    "* Convex Optimization (Boyde Vandenberghe), Cambridge University Press, 2004\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "Optimization is an ubiquitous tool in economics, mathematics, and coding. The Math+Econ+Code masterclass heavily relies on optimization techniques, which we will introduce in this lecture.\n",
    "\n",
    "First, we will write the very general formulation of a constrained optimization problem in $\\mathbb{R}^n$, which will be relevant for most of the applications you'll see. Second, we will review techniques for unconstrained, equality and inequality constrained optimization problems, in static terms. Dynamic optimization will be considered in the third lecture of these prerequisites.\n",
    "\n",
    "Finally, we will consider more conceptual concepts such as duality and linear programming, which are core to understanding and using Optimal Transport.\n",
    "\n",
    "I will assume that you have basic knowledge of optimization. I will not re-define formally simple concepts like local/global maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The optimization problem\n",
    "\n",
    "In general, a maximization problem takes the following form:\n",
    "\n",
    "\\begin{align}\n",
    "\\max_{x\\in A}  & \\, f(x) \\\\\n",
    "s.t: & \\ {g}(x) = d, \\\\\n",
    "& \\ h(x) \\leq e\n",
    "\\end{align}\n",
    "\n",
    "$f(x)$ is the objective function. $A$ is the feasible set.\n",
    "The goal of the optimization process is to find some $x^* \\in A$ such that $f(x^*) \\geq f(x),  \\forall x \\in A$\n",
    "\n",
    "We can account for three constraints: $x \\in A$, ${g}(x) = d$ and $h(x) \\leq e$. You should be aware that $x$ is a vector. Usually, $x$, $d$ and $e$ $\\in \\mathbb{R}^n$, **g** and **h** are set of multiple constraints. For example, if we take the vector $x$ = $[x_1, x_2]$ where $x \\in \\mathbb{R}^2$, we can write this simple optimization problem taken from economics:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\max_{x\\in \\mathbb{R}^2}  & \\, x_1^\\frac{1}{2} \\cdot x_2^\\frac{1}{2} \\\\\n",
    "s.t: & \\ 2x_1 + 3x_2 = 12, \\\\\n",
    "& \\ x_1 \\geq 0 \\\\\n",
    "& \\ x_2 \\geq 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This corresponds to the consumer utility maximisation problem every economics undergrad is familiar with. So here, we have one equality constraint, corresponding to the consumer budget constraint, and the two inequalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving an optimization problem\n",
    "#### The existence conditions\n",
    "\n",
    "It is useless to look for an optimum of a function if this optimum does not exist: typically, if you were to look for the unconstrained maximum of the function $f(x) = x^2$, you would only get $+\\infty$. The core result that should be the first step of every optimization problem is the Weierstrass Theorem.\n",
    "\n",
    "\n",
    "Weierstrass Theorem: \n",
    "Let $D \\subset \\mathbb{R}^n$ be a compact space. Let $f: D \\rightarrow \\mathbb{R}$ be a continuous function on $D$, then $f$ attains a maximum and a minimum on $D$. \n",
    "\n",
    "Be careful: the conditions laid down in this theorem are not necessary, but only sufficient. If they are met, we are sure that there exists a max and a min. If they are not met, that doesn't mean that we can't find a max and/or a min.\n",
    "\n",
    "Let's unpack this theorem a bit: while a continuous function is rather straightforward, what is a compact space ? Mathematicians use a very formal and topological definition. Us economists, we use this simpler version that works in the restricted setting of Euclidean spaces.\n",
    "\n",
    "* A subset $S \\subset \\mathbb{R}^n$  is compact if it is closed and bounded (Heine-Borel theorem) .\n",
    "\n",
    "It is easy to see whether a set is closed or open in $\\mathbb{R}^n$. As a reminder, if a set $X$ is open, then $\\forall x \\in X$, there exists an open ball of radius $r$ and centered on $x$, $B_r(x)$ included in $X$: $B_r(x) \\subset X$. Reminder: in $\\mathbb{R}^2$, an open ball of finite radius $r$ is a circle centered on (0, 0). A set $Y$ is then said to be **closed** if its complement $Y^c = \\mathbb{R}^n \\setminus Y$ is open. Using this definition, $[0, 1]$ is closed, while $(0, 1)$ is open. Particular cases, the sets $\\emptyset$ and $R^n$ are both open and closed sets. Thus they possess the properties we are looking for. \n",
    "\n",
    "* A subset $S \\subset \\mathbb{R}^n$ is bounded when there exists $r > 0$ such that $S \\subset B_r(0)$, where $B_r(0)$ again designates the **open ball** of radius $r$, centered on $0$. If, for a circle with radius large enough, you can fit the set inside the ball, then your set is bounded. For example, $[0, 1] \\times [0, 1]$ is a bounded set, while $\\mathbb{R}^2$ is not. Can you see why ?\n",
    "\n",
    "\n",
    "So, practical guide for economists: boxes to check.\n",
    "\n",
    "* Is the function continuous ?\n",
    "* Is the set over which I am optimizing closed ?\n",
    "* Can I fit this set inside an open ball ?\n",
    "\n",
    "If you can answer yes to these three questions, you can be sure that there exists a minimum and a maximum to your optimization problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of the technique.\n",
    "\n",
    "The technique to use to solve the problem at hand depends on its type. We restrict our attention here on problems that are actually solvable with pen and paper. Problems that can't be solved this way require numerical optimization techniques, which we will study tomorrow. Techniques are usually divided in 2 steps: first-order conditions and second-order conditions.\n",
    "\n",
    "* No constraints, you are optimizing over $\\mathbb{R}^n$: Unconstrained optimization $\\Rightarrow$ FOC, SOC\n",
    "* Equality constraints, constraints take the form $f(x) = d$: Theorem of Lagrange\n",
    "* Inequality constraints (or equality **and** inequality constraints): Karush-Kuhn-Tucker (KKT) theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconstrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with unconstrained optimization, for simplicity in $\\mathbb{R}^2$ (but the generalization to $\\mathbb{R}^n$ is straightforward). The problem takes the following form:\n",
    "\\begin{align}\n",
    "\\max_{(x, y) \\in R^2}  & \\, f(x, y) \\\\\n",
    "\\end{align}\n",
    "\n",
    "FOC: We take the **Jacobian**: $Df(x, y) = \\big(\\frac{\\partial f(x, y)}{\\partial x}, \\frac{\\partial f(x, y)}{\\partial y}\\big)$\n",
    "Each element of the Jacobian must be equal to $0$ at the optimum. The $n$ partial derivatives of a function in $n$ variables form a system of $n$ equations. But the solution might be a min, a max, or a saddle point. There may be several solutions to this system, some of which may be only local optimizers.\n",
    "\n",
    "SOC: Easy procedure: use intelligence and look at the function. If it is a composite function of concave function, like $x^4 + 3x^2 - 2x + 8$, you do not need to check the Hessian to know that the function is concave. If the candidate optimum found with FOC is unique, then you have your minimum. If it is not possible to observe easily the concavity/convexity of the objective function, you need to take the more cumbersome procedure described here: \n",
    "\n",
    "Formal procedure: We take the **Hessian**: $D^2f(x, y) = \\Bigg( \\begin{matrix} \n",
    "\\frac{\\partial^2 f(x, y)}{\\partial x^2} & \\frac{\\partial^2 f(x, y)}{\\partial x \\partial y}\\\\\n",
    "\\frac{\\partial^2 f(x, y)}{\\partial y \\partial x} & \\frac{\\partial^2 f(x, y)}{\\partial y^2}\n",
    "\\end{matrix}\\Bigg)$\n",
    "\n",
    "If you have several, say $k$, $(x^*, y^*)$ points such that $Df(x^*, y^*) = (0, 0)$, you should compute $k$ Hessians $D^2f(x^*, y^*)$ which are matrix of numbers. If the Hessian is a negative (positive) definite matrix, the point $(x^*, y^*)$ is a strict local maximum (minimum). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly, what is a positive definite matrix ? One of the way to check it is the following: For a given matrix A = $\\Bigg( \\begin{matrix} \n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{matrix}\\Bigg)$\n",
    "\n",
    "\"Leading principal minors\" are the determinants of submatrices $a_{11}$, $\\Big(\\begin{matrix} \n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{matrix}\\Big)$, and A itself. If the determinants of these three submatrices are positive, $A$ are positive definite. If you are comfortable computing eigenvalues of a matrix, you should know that a matrix is positive (negative)definite $\\Leftrightarrow$ its eigenvalues are strictly positive (negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality constrained optimization: Lagrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the presentation above was quite formal and general for unconstrained optimization, we will move to a more informal style for Lagrange and KKT.\n",
    "\n",
    "A simplified consumer problem relies on the fact that agents spend all of their income to consume. This way, the budget constraint is made binding (agents do not waste money). We will solve this problem using the Lagrangian. The simplification is allowed because we know that people will not consume negative quantities, and will not waste money. So, we may simplify the problem written in the beginning of these notes as:\n",
    "\n",
    "\\begin{align}\n",
    "\\max_{x\\in \\mathbb{R}^2}  & \\, x_1^\\frac{1}{2} \\cdot x_2^\\frac{1}{2} \\\\\n",
    "s.t: & \\ 2x_1 + 3x_2 = 12\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The Lagrangian technique consists in rewriting the problem as an unconstrained one, in the following way.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} =  x_1^\\frac{1}{2} \\cdot x_2^\\frac{1}{2} + \\lambda [12 - 2x_1 - 3x_2]\n",
    "\\end{align}\n",
    "\n",
    "We take the FOC, w.r.t $x_1$, $x_2$ and $\\lambda$:\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial x_1} = 0$\n",
    "* $\\frac{\\partial \\mathcal{L}}{\\partial x_2} = 0$\n",
    "* The partial derivative wrt $\\lambda$, equalized to 0, simply yields the constraint\n",
    "\n",
    "We can solve this system of equations for $x_1$, $x_2$ and $\\lambda$, which yields a candidate point. Given that the Weierstrass theorem applies, there is no need to check the SOC. If the Weierstrass theorem does not apply, you may try to look at the function to check that it is globally concave/convex or try a more cumbersome procedure similar to the one used for unconstrained optimization.\n",
    "\n",
    "One final note on $\\lambda$: in the consumer utility maximization problem, it is the **shadow price**. It can be interpreted as the additional utility that would be achieved, were the constrained be relaxed by one unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumbersome procedure mentioned above is the following: \n",
    "* Compute the bordered Hessian: it is composed of four blocks which can be matrices/vectors. For easier notation, I drop the $(x, \\lambda)$ after each $H$ or $\\mathcal{L}$\n",
    "\\begin{align}\n",
    "H(x, \\lambda) = \\Bigg( \\begin{matrix} \n",
    "\\frac{\\partial^2 \\mathcal{L}}{\\partial \\lambda^2} & \\frac{\\partial^2 \\mathcal{L}}{\\partial \\lambda \\partial x}\\\\\n",
    "\\frac{\\partial^2 \\mathcal{L}}{\\partial x \\partial \\lambda} & \\frac{\\partial^2 \\mathcal{L}}{\\partial x^2}\n",
    "\\end{matrix}\\Bigg)\n",
    "\\end{align}\n",
    "\n",
    "In the example above, the bloc on the top-left would be 1x1, top-right would be 1x2, bottom-left 2x1 and bottom-right 2x2.\n",
    "\n",
    "* Say the optimizing vector $x$ is of dimension $n$ (above, $n$ = 2) and there are $m$ binding constraints (here, $m$ = 1).\n",
    "* Starting from the smallest (in size/dimension), take the $(n-m)$ largest (in size) leading principal minors of the Hessian and compute them (as defined earlier)\n",
    "* If their signs alternate, starting from the sign of $(-1)^{m+1}$, then the candidate point is a local maximizer\n",
    "* If their signs all have signs $(-1)^m$, then the candidate point is a local minimizer.\n",
    "\n",
    "As you can see, this procedure is very cumbersome. It should be done only as a last resort when no other procedure can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inequality constrained optimization: Karush-Kuhn-Tucker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the constraint is an inequality one, an additional issue arises: some of the constraints may not be binding at the optimum. For example, in the following problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{x\\in \\mathbb{R}^2}  & \\, x_1^2 + x_2^2 \\\\\n",
    "s.t: & \\ x_1 + x_2 \\leq 3\n",
    "\\end{align}\n",
    "\n",
    "It is quite clear the the optimum is (0, 0). The point-vector (0, 0) satisfies the constraint, which is not binding at the optimum ($0 + 0 \\neq 3$). If we attempted to solve this problem like the equality constrained one, we would not obtain the actual minimum.\n",
    "\n",
    "The KKT procedure requires that special care is attributed to signs. I suggest that you always use the following procedure in order to avoid mistakes, by rewriting the problem with a specific form:\n",
    "\n",
    "**Maximization problem**:\n",
    "\\begin{align}\n",
    "\\max_{x\\in \\mathbb{R}^n}  & \\, f(x) \\\\\n",
    "s.t: & \\ g(x) \\leq b\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Minimization problem**:\n",
    "\\begin{align}\n",
    "\\min_{x\\in \\mathbb{R}^n}  & \\, f(x) \\\\\n",
    "s.t: & \\ g(x) \\geq b\n",
    "\\end{align}\n",
    "\n",
    "Where $b \\in \\mathbb{R}$ can be $0$. You should rewrite all the constraints in a given problem with this structure.\n",
    "\n",
    "First, we formumate the Lagrangian as before:\n",
    "$\\mathcal{L} = f(x) + \\lambda [b - g(x)]$\n",
    "\n",
    "\n",
    "Then, the KKT equations allowing you to solve the problem are the following:\n",
    "\n",
    "\\begin{align}\n",
    "D\\mathcal{L} = 0 \\ & (FOC) \\\\\n",
    "\\lambda \\geq 0\\\\\n",
    "\\lambda \\cdot (b - g(x)) = 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Note that $\\lambda$ refers here to a vector, of dimension equal to the number of constraints. The last expression is called the complementary constraints. It reflects the fact that either $\\lambda$ or $b - g(x)$ is equal to 0. In other words, if $\\lambda_{(j)}$ is different from 0, then constraint $j$ is binding. Solving this system of equations yields the optimal point. Again, SOC should be checked, either by applying the Weierstrass theorem, looking at the objective function, or following the cumbersome procedure defined above for equality constrained optimization problems, taking only into account the binding constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math break: a tool for optimization: SymPy\n",
    "\n",
    "Instead of using the well-known Wolfram Alpha to perform symbolic calculus, you may use Python with just a little bit of programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unconstrained optimization with SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = symbols('x1 x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us say we want to minimize the following function: $f(x_1, x_2) = (x_1 - 1)^2 + x_1 x_2 + x_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x_{1} x_{2} + x_{2}^{2} + \\left(x_{1} - 1\\right)^{2}$"
      ],
      "text/plain": [
       "x1*x2 + x2**2 + (x1 - 1)**2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = (x1-1)**2 + x2**2 + x1*x2\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dobj_dx1 = diff(obj, x1)\n",
    "dobj_dx2 = diff(obj, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2*x1 + x2 - 2, x1 + 2*x2], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian = np.array([dobj_dx1, dobj_dx2])\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{x1: 4/3, x2: -2/3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(([jacobian[0], jacobian[1]]), (x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1.],\n",
       "       [1., 2.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hessian\n",
    "hessian = np.array([diff(dobj_dx1, x1), diff(dobj_dx2, x1), diff(dobj_dx1, x2), diff(dobj_dx2, x2)], dtype='float').reshape(2, 2)\n",
    "hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since the problem is a minimization one, we need the objective function to be convex \n",
    "# => the hessian matrix should be positive definite. An easy way to do so using numpy is to check the eigenvalues.\n",
    "\n",
    "np.linalg.eigvals(hessian)\n",
    "\n",
    "# Since all eigenvalues of the Hessian are positive, the Hessian is indeed positive definite. So, the candidate point (0, 0)\n",
    "# is correctly the minimum of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More advanced topics in static optimization: a brief introduction to Linear Programming and Duality\n",
    "\n",
    "Reference: Optimal Transport Methods in Economics (Galichon), Appendix B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Linear Programming\n",
    "\n",
    "Linear Programming applies to a subset of optimization problems in which the objective function as well as all the constraints are linear function. Unlike nonlinear/non-convex optimization problems, linear programming problems can be solved relatively easily with numerical solvers such as Gurobi, which is used intensively in Math+Econ+Code. In particular, Optimal Transportation problems can be reformulated as Linear Programming (LP) problems.\n",
    "\n",
    "Example:\n",
    "\n",
    "\\begin{align}\n",
    "\\max_{x\\in \\mathbb{R}^3}  & \\, 4x_1 + 3x_2 - x_3 \\\\\n",
    "s.t: & \\ -x_1 + 8x_2 - 4x_3 = 3 \\\\\n",
    "& \\ 3x_1 + x_2 + 2x_3 = -5\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This type of optimization problem can be rewritten in a general form as:\n",
    "\n",
    "\\begin{align}\n",
    "V_p = \\max_{x\\in \\mathbb{R}^n}  & \\, x'c \\\\\n",
    "s.t: & \\ Ax = b \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where $c$, $x$ and $b$ are vectors and $A$ is a matrix.\n",
    "\n",
    "In the example above,\n",
    "$x = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}$, $c = \\begin{bmatrix}\n",
    "4 \\\\\n",
    "3 \\\\\n",
    "-1\n",
    "\\end{bmatrix}$, \n",
    "$A = \\begin{bmatrix}\n",
    "-1 & 8 & -4 \\\\\n",
    "3 & 1 & 2\n",
    "\\end{bmatrix}$, $b = \\begin{bmatrix}\n",
    "3 \\\\\n",
    "-5\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duality\n",
    "Now, what is duality ? A perspective on optimization problems that yields interesting results when we apply it to topics studied in the masterclass (optimal assignment problems).\n",
    "\n",
    "Every optimization problem can be studied from two different perspectives: the primal and the dual. So far, we have exclusively adopted the perspective of solving the **primal** problem. But any problem could be rewritten in another form, through the Lagrangian. Duality can be used for any optimization problems, but the interesting thing about the **dual** of LP problems is that the solution of the dual and the primal coincide if it exists.\n",
    "\n",
    "As you can see if you study convex optimization further (see Boyde Vandenberghe), the dual and the primal coincide for almost every \"convex\" problem. It should also be noted that even when the primal is nonconvex (and therefore hard to solve), the dual will be convex. Although the solutions do not coincide then, the value of the dual provides a lower bound on the optimal value of the primal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's derive the dual of the abstract LP problem written above: first, we rewrite the problem as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\max_x c'x + F(d - Ax)\n",
    "\\end{align}\n",
    "\n",
    "F is a function that takes the difference between the two terms of the constraint. We want the constraint to be met, so we are very unhappy if $(d - Ax)$ is different from $0$. The consequence of this unhappiness, in a maximization problem, is that $F$ returns $-\\infty$. Otherwise, if $(d-Ax)$ is exactly equal to $0$, we are happy, but we want the value of the max to be informative (not $+\\infty$). So, in this case $F$ returns 0. To summarize:\n",
    "\n",
    "\\begin{align}\n",
    "F(d-Ax) = \\Bigg\\{ \\begin{matrix} 0 & if & (d-Ax) = 0 \\\\ -\\infty &  if & (d-Ax) \\neq 0 \\end{matrix} \n",
    "\\end{align}\n",
    "\n",
    "What would be a good candidate for this function ?\n",
    "\n",
    "\\begin{align}\n",
    "F(d-Ax) = \\min_y y'(d - Ax)\n",
    "\\end{align}\n",
    "\n",
    "Indeed, if $(d - Ax)$ is equal to $0$ (binding constraint), the product $y'(d - Ax)$ is equal to $0$ no matter what the value of $y$ is. Otherwise, if $(d - Ax) > 0$, we can minimize by fixing $y$ arbitrarily low, and F yields $-\\infty$ like we want. If $(d - Ax) < 0$, we fix $y$ arbitrarily high and F yields again $-\\infty$.\n",
    "\n",
    "Let us rewrite our **primal** problem as follows:\n",
    "\n",
    "\\begin{align}\n",
    "V_p\n",
    "& = \\max_x \\Big\\{x'c + \\min_y y'(d - Ax)\\Big\\} \\\\\n",
    "& = \\max_x \\min_y \\Big\\{x'c + y'd - y'Ax\\Big\\} \\\\\n",
    "& = \\min_y \\max_x \\Big\\{x'c + y'd - y'Ax \\Big\\} \\\\\n",
    "& = \\min_y y'd + \\max_x \\Big\\{x'c - x'A'y \\Big\\} \\\\\n",
    "& = \\min_y y'd + \\max_x \\Big\\{x'(c - A'y)\\Big\\} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "= \\min y'd \\\\\n",
    "s.t: & \\ A'y = c \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new linear problem is called the **dual**. We denote its value $V_d$.\n",
    "\n",
    "Note that we used $\\max_x \\min_y = \\min_y \\max_x$ a bit quickly. In general, for a maximization problem, the value of the primal is weakly lower than the value of the dual:\n",
    "\n",
    "\\begin{align}\n",
    "V_p &\\leq V_d \\\\\n",
    "\\max_x \\min_y &\\leq \\min_y \\max_x\n",
    "\\end{align}\n",
    "\n",
    "In fact, the equality between the two is allowed by the Minimax theorem (see OTME Appendix B or [here](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_du_minimax_de_von_Neumann)): either both the primal and the dual are feasible, and $V_p = V_d$ holds, either both problems are infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: write the dual of the linear programming example written at the beginning of this part.\\\\\n",
    "\n",
    "Exercise 2: solve the original LP problem using Scipy solver [linprog](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
