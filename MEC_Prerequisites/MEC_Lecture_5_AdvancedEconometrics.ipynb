{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math+Econ+Code Prerequisites 5: Introduction to BLP and Rust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last session, we will study two advanced econometrics topics that will be studied in the M+E+C masterclass. The masterclass is assuming knowledge of the principles for these two techniques, and will re-visit them using Optimal Transport. Today, we will build the basics for these two tools and introduce two fields of economics in the process:\n",
    "* B(erry)-L(evinsohn)-P(akes): a structural model in industrial organization\n",
    "* Rust: a structural model for dynamic discrete choice\n",
    "\n",
    "These two objects are kept for the end because you now have all the intermediate tools that are necessary to understand them, and the mainstream algorithm that can be used for both of them has a similar structure called: nested fixed point (NFXP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* \"Automobile Prices in Market Equilibrium\", (Berry, Levinsohn and Pakes), Econometrica Vol. 63, No. 4 (Jul., 1995), pp. 841-890\n",
    "* \"Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher\", (John Rust), Econometrica, Vol. 55, No. 5 (Sep., 1987), pp. 999-1033\n",
    "* \"Best practices for differentiated products demand estimation with PyBLP\", (Conlon and Gortmaker), The RAND Journal of Economics, volume 51, No. 4, 2020\n",
    "* Nested Fixed Point Algorithm Documentation Manual (Rust), version 6, 2000\n",
    "\n",
    "\n",
    "The data used here and the code for Rust was taken and adapted from the following Rust replication: we invite you to take a look at the original notebook for a more advanced replication of the Rust paper and a good tutorial on how to handle the data from the original Rust paper: https://notes.quantecon.org/submission/6234fe0f96e1ce001b61fad8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLP: Random Coefficients Logit Model\n",
    "\n",
    "Utility function is specified as follows, where $i$ indexes individuals, and $j$ indexes alternatives:\n",
    "\n",
    "\\begin{align}\n",
    "u_{ij} = x_j'\\beta_i - p_j + \\xi_j + \\epsilon_{ij}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_j$ is a vector of characteristics specific to alternative $j$\n",
    "* $p_j$ is the price of alternative $j$\n",
    "* $\\xi_j$ is the unobserved heterogeneity specific to characteristic $j$\n",
    "* $\\epsilon_{ij}$ is the econometric error of the model, which we assumed to be Gumbel distributed.\n",
    "\n",
    "How do these differ from the models you are used to ? The coefficient $\\beta$ is indexed per individual. Denoting $\\overline{\\beta}$ the mean of this \"random coefficient\", and considering that the vector $x_j$ contains $k$ characteristics:\n",
    "\n",
    "\\begin{align}\n",
    "x_j' \\beta_i = x_j \\overline{\\beta} + \\sum_{k=1}^K \\sigma_k x_{jk} \\nu_{ik}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\nu_{ik} \\sim \\mathcal{N}(0, 1)$ is an individual-characteristic idiosyncratic shock\n",
    "* $\\overline{\\beta}_k$ and $\\sigma_k$ are the parameters to estimate\n",
    "\n",
    "\\begin{align*}\n",
    "u_{ij} &= x_j \\overline{\\beta} - \\alpha p_j + \\xi_j + \\sum_{k=1}^K \\sigma_k x_{jk} \\nu_{ik} + \\epsilon_{ij} \\\\\n",
    "&= \\delta_j + \\sum_{k=1}^K \\sigma_k x_{jk} \\nu_{ik} + \\epsilon_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\delta_j$ denotes the \"mean utility\" that alternative $j$ provides to individuals. The second object $\\sum_{k=1}^K \\sigma_k x_{jk} \\nu_{ik}$ allows us to let individuals have correlated tastes for product **characteristics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given individual $i$, we can use the formula above to rewrite the choice probability of that individual using the multinomial logit form you know already:\n",
    "\n",
    "\\begin{align*}\n",
    "P_{ij} &= \\frac{\\exp(\\delta_j + \\sum_{k=1}^K \\sigma_k x_{jk} \\nu_{ik})}{1 + \\sum_{h=1}^J \\exp(\\delta_h + \\sum_{k=1}^K \\sigma_k x_{hk} \\nu_{ik})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only uncertain component in this expression is $\\nu_{i}$, which varies across individuals. Therefore, the \"market share\" of alternative $j$, when integrated across the population, can be written as follows:\n",
    "\n",
    "\\begin{align}\n",
    "s_j = \\int P_{ij}(\\nu_i) f_\\nu(\\nu_i) d\\nu_i\n",
    "\\end{align}\n",
    "\n",
    "This integral is not necessarily easy to compute exactly: in their paper, BLP relies on simulation to compute it. Briefly, let's review the mechanism of the algorithm before coding it ourselves.\n",
    "\n",
    "One last element that's required for BLP: instruments. Indeed, prices are endogenous in this model, which is why we need to use some sort of IV. If you do not know IV, check it in \"Mostly Harmless Econometrics\". Instruments need to have two main properties to be usable: they need to be a good predictor of the endogenous variable (here, the price), and be uncorrelated with the variable we attempt to explain (here, $u_{ij}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Start from an arbitrary value $\\theta_0 = (\\beta_0, \\sigma_0)$\n",
    "* Set $\\hat{\\theta} = \\theta_0$\n",
    "    * Using $\\hat{\\theta}$, compute mean utilities $\\hat{\\delta_j}$ and simulate market shares $\\tilde{s}_j$\n",
    "    * Through a logit regression, recover the unobserved heterogeneity that corresponds to the current guess of $\\theta$: $\\hat{\\xi}(\\hat{\\theta}) = \\hat{\\delta_{jt}} + \\alpha p_{jt} - x_{jt} \\hat{\\overline{\\beta}}$\n",
    "    * Compute $E[\\hat{\\xi}(\\theta)'Z] = \\frac{\\sum_{j, t} z'_{jt} \\hat{\\xi}_{jt}(\\theta)}{N}$. If it is equal to 0: we have found the correct $\\theta = \\hat{\\theta}$. Otherwise: update $\\hat{\\theta}$ and go back to step 1. (IV GMM estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will do this simply, using the pyblp library.\n",
    "#Coding this by hand is quite cumbersome.\n",
    "\n",
    "#!pip install pyblp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyblp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['market_ids', 'city_ids', 'quarter', 'product_ids', 'firm_ids',\n",
       "       'brand_ids', 'shares', 'prices', 'sugar', 'mushy',\n",
       "       'demand_instruments0', 'demand_instruments1', 'demand_instruments2',\n",
       "       'demand_instruments3', 'demand_instruments4', 'demand_instruments5',\n",
       "       'demand_instruments6', 'demand_instruments7', 'demand_instruments8',\n",
       "       'demand_instruments9', 'demand_instruments10', 'demand_instruments11',\n",
       "       'demand_instruments12', 'demand_instruments13', 'demand_instruments14',\n",
       "       'demand_instruments15', 'demand_instruments16', 'demand_instruments17',\n",
       "       'demand_instruments18', 'demand_instruments19'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this (fake) cereal data is provided directly by the pyblp library to try out their code\n",
    "\n",
    "product_data = pd.read_csv(pyblp.data.NEVO_PRODUCTS_LOCATION)\n",
    "product_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short tutorial in pyblp\n",
    "\n",
    "pyblp is a good tool to master if you are interested in industrial organization (IO) or if you want to RA for a professor who does IO\n",
    "\n",
    "\n",
    "* Load the data as a Pandas object (done already)\n",
    "* Write a $X_1$ formulation which contains all variables for which you do not want random coefficients (in the example below, there is no constant $(0)$, but we add product fixed effect\n",
    "* Write a $X_2$ formulation, which includes every variable for which you want random coefficients $(1)$ is for the Constant\n",
    "* Select a method for simulating the market shares\n",
    "* write a pyblp.Problem object which aggregates the regression formulation, data and integration method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the problem ...\n",
      "Absorbing demand-side fixed effects ...\n",
      "Initialized the problem after 00:00:00.\n",
      "\n",
      "Dimensions:\n",
      "============================================\n",
      " T    N     F    I     K1    K2    MD    ED \n",
      "---  ----  ---  ----  ----  ----  ----  ----\n",
      "94   2256   5   4700   1     4     20    1  \n",
      "============================================\n",
      "\n",
      "Formulations:\n",
      "===========================================================\n",
      "       Column Indices:           0       1       2      3  \n",
      "-----------------------------  ------  ------  -----  -----\n",
      " X1: Linear Characteristics    prices                      \n",
      "X2: Nonlinear Characteristics    1     prices  sugar  mushy\n",
      "===========================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dimensions:\n",
       "============================================\n",
       " T    N     F    I     K1    K2    MD    ED \n",
       "---  ----  ---  ----  ----  ----  ----  ----\n",
       "94   2256   5   4700   1     4     20    1  \n",
       "============================================\n",
       "\n",
       "Formulations:\n",
       "===========================================================\n",
       "       Column Indices:           0       1       2      3  \n",
       "-----------------------------  ------  ------  -----  -----\n",
       " X1: Linear Characteristics    prices                      \n",
       "X2: Nonlinear Characteristics    1     prices  sugar  mushy\n",
       "==========================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X1: Linear variables, no random coefficients\n",
    "#X2: Nonlinear variables, random coefficients\n",
    "\n",
    "X1_formulation = pyblp.Formulation('0 + prices', absorb='C(product_ids)')\n",
    "X2_formulation = pyblp.Formulation('1 + prices + sugar + mushy')\n",
    "\n",
    "product_formulations = (X1_formulation, X2_formulation)\n",
    "\n",
    "\n",
    "#Numerical integration of the probability P_{ij}\n",
    "mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed': 0})\n",
    "\n",
    "mc_problem = pyblp.Problem(product_formulations, product_data, integration=mc_integration)\n",
    "mc_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Problem Results Summary:\n",
       "==================================================================================================================\n",
       "GMM     Objective      Projected    Reduced Hessian  Reduced Hessian  Clipped  Weighting Matrix  Covariance Matrix\n",
       "Step      Value      Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares   Condition Number  Condition Number \n",
       "----  -------------  -------------  ---------------  ---------------  -------  ----------------  -----------------\n",
       " 2    +1.519665E+02  +3.454863E-05   -7.107164E-13    +7.151339E+03      0      +5.511345E+07      +4.728118E+05  \n",
       "==================================================================================================================\n",
       "\n",
       "Cumulative Statistics:\n",
       "===========================================================================\n",
       "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
       "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
       "-----------  ---------  ------------  -----------  -----------  -----------\n",
       " 00:01:44       No          214           290        167366       522877   \n",
       "===========================================================================\n",
       "\n",
       "Nonlinear Coefficient Estimates (Robust SEs in Parentheses):\n",
       "=================================================================================================================================================================\n",
       "Sigma:         1             prices            sugar            mushy       |  Sigma Squared:         1             prices            sugar            mushy     \n",
       "------  ---------------  ---------------  ---------------  ---------------  |  --------------  ---------------  ---------------  ---------------  ---------------\n",
       "  1      +0.000000E+00                                                      |        1          +0.000000E+00    +0.000000E+00    +0.000000E+00    +0.000000E+00 \n",
       "        (+2.467348E+00)                                                     |                  (+0.000000E+00)  (+1.964048E+01)  (+2.546949E-01)  (+5.657851E-01)\n",
       "                                                                            |                                                                                    \n",
       "prices   +7.960159E+00    +0.000000E+00                                     |      prices       +0.000000E+00    +6.336413E+01    -8.216967E-01    -1.825336E+00 \n",
       "        (+7.184391E+00)  (+1.748100E+01)                                    |                  (+1.964048E+01)  (+1.143778E+02)  (+9.607968E-01)  (+1.115449E+01)\n",
       "                                                                            |                                                                                    \n",
       "sugar    -1.032262E-01    +2.865070E-02    +3.910383E-02                    |      sugar        +0.000000E+00    -8.216967E-01    +1.300561E-02    +1.637189E-02 \n",
       "        (+1.410812E-01)  (+2.195831E-01)  (+7.759442E-02)                   |                  (+2.546949E-01)  (+9.607968E-01)  (+3.296601E-02)  (+2.565018E-01)\n",
       "                                                                            |                                                                                    \n",
       "mushy    -2.293090E-01    -3.562613E-01    +7.437476E-02    +5.318510E-01   |      mushy        +0.000000E+00    -1.825336E+00    +1.637189E-02    +4.679019E-01 \n",
       "        (+1.464605E+00)  (+3.168498E+00)  (+1.580627E+00)  (+2.213390E+00)  |                  (+5.657851E-01)  (+1.115449E+01)  (+2.565018E-01)  (+2.345518E+00)\n",
       "=================================================================================================================================================================\n",
       "\n",
       "Beta Estimates (Robust SEs in Parentheses):\n",
       "===============\n",
       "    prices     \n",
       "---------------\n",
       " -3.077327E+01 \n",
       "(+3.044823E+00)\n",
       "==============="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyblp.options.verbose = False\n",
    "results = mc_problem.solve(sigma=np.ones((4, 4)))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rust 1987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rust is one of the first models using Dynamic Discrete Choice. So far, we have done static Discrete Choice, where an individual's choice is completely described by the alternatives available. Here, every choice has consequences on the future. Therefore, we will combine dynamic programming and discrete choice models to estimate the structural parameters in a dynamic problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Discrete Choice\n",
    "In a quite general form, the dynamic discrete choice problem can be written this way. We focus here on an infinite horizon case, so the policy will be stationary. We denote present state $x$ and next period state $x'$.\n",
    "\n",
    "\\begin{align}\n",
    "V_\\theta(x,\\epsilon) &= \\max_{d \\in D(x)} \\Big[ u(x, d, \\theta) + \\epsilon(d) + \\beta\\int_{x'} \\int_{\\epsilon'} V_\\theta(x, \\epsilon) \\cdot π(x',\\epsilon'|x, \\epsilon, \\theta) dx' d\\epsilon' \\Big]\\\\\n",
    "V_\\theta(x,\\epsilon) &= \\max_{d \\in D(x)} \\Big[ u(x, d, \\theta) + \\epsilon(d) + \\beta E[V_\\theta(x, \\epsilon, d) ] \\Big]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose this expression: when the agent is at state $x$, he faces a set of possible actions: $D(x)$. Given his decision, he will get a flow of immediate utility $u(x, d, \\theta)$, some random shock $\\epsilon(d)$, and a discounted value for future state $\\beta E[V_\\theta(x, \\epsilon, d) ]$.\n",
    "\n",
    "As you see there is some randomness in the value of future state. We integrate two times for the two sources of randomness: taking action $d$ does not guarantee us to get some state $x'$, and we ignore the value of the future random shock $\\epsilon'$. So far this is a lot of uncertainty, and we can hardly go further. Thus we make two assumptions:\n",
    "\n",
    "* $\\epsilon \\sim_{iid} \\text{Gumbel}(0, 1)$\n",
    "* $\\pi(x'\\epsilon' | x, e, \\theta) = p(x'|x, d, \\theta) \\cdot q(\\epsilon'|x, \\theta)$\n",
    "\n",
    "What do these mean ? We can separate the state transition from the random shock, and the distributional assumption on $\\epsilon$ implies that we'll be able to rewrite the expected value function in close form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Emax operator\n",
    "Small parenthesis that will be useful anytime you tackle this type of problem. Let there be $j$ alternatives that provide systematic utility $U_j$ and a random shock $\\epsilon_j$:\n",
    "\n",
    "\\begin{align}\n",
    "E\\big[ \\max \\{U_j + \\epsilon_j\\} \\big] = \\gamma + \\ln \\sum_j e^{U_j}\n",
    "\\end{align}\n",
    "\n",
    "Where $\\gamma = E[\\epsilon] \\approx 0.57$, the Euler-Mascheroni constant.\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align}\n",
    "&E\\big[V_\\theta (x)\\big]\\\\ \n",
    "= &E\\Big[ \\max_{d∈D(x)} u(x, d, \\theta) + \\epsilon + \\beta E[V_\\theta(x')]\\Big] \\\\\n",
    "= &\\int_{x} \\Big[ \\gamma + \\log \\big[ \\sum_{d \\in D(x)} \\exp\\{ u(x, d, \\theta) + \\beta E[V_\\theta(x')\\} \\big]\\Big] p(x|d, \\theta) dx\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we integrate the $\\epsilon$ uncertainty and rewrite the value function:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "V_\\theta(x) &= \\max_{d∈D(x)} \\Big[ u(x, d, \\theta) + \\epsilon + \\beta E[V_\\theta(x')\\Big]\\\\\n",
    "&= \\\\\n",
    "&\\max_{d∈D(x)} \\Big[ u(x, d, \\theta) + \\epsilon + \\beta\\int_{x'} \\Big[ \\gamma +\\log \\big[ \\sum_{d \\in D(x')} \\exp\\{ u(x', d, \\theta) + \\beta E[V_\\theta(x'', d)] \\big] \\Big] p(x'|d, \\theta) dx'\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our session on multinomial logit: we can use McFadden's formula: if alternative $j$ provides utility $U_{ij} = V_{ij} + \\epsilon_{ij}$ to consumer $i$, the probability that he chooses alternative $j$ can be written: \n",
    "\n",
    "\\begin{align}\n",
    "P_{ij} = P(j = \\arg \\max U_{ij}) = \\frac{e^{V_{ij}}}{\\sum_k e^{V_{ik}}}\n",
    "\\end{align}\n",
    "\n",
    "Here, the equivalent of $U_{ij}$ in a dynamic setting is $V_\\theta(x, d)$. Therefore, the **Conditional Choice Probabilities** are written as follows:\n",
    "\n",
    "\\begin{align}\n",
    "P(d|x, \\theta) = \\frac{\\exp\\{{u(x, d, \\theta) + \\beta E[V_\\theta(x', d)]}\\}}{\\sum_{k \\in D(x)} \\exp\\{{u(x', k, \\theta) + \\beta E[ V_\\theta(x', k)] }\\}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now take an econometrics perspective: we can observe $(x, d)_{\\{i, t\\}}$, but not $\\theta$. The estimation of $\\hat{\\theta}$ can be done through maximum likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "LL(\\theta) = \\sum_t \\sum_i \\sum_j d_j \\log \\big( P(d_{ijt}|x_j, \\theta) \\big)\n",
    "\\end{align}\n",
    "\n",
    "Where $d_j$ is an indicator equal to $1$ if agent $i$ chose $j$ at time $t$. We can then find the ML estimate $\\hat{\\theta} = \\arg \\max_\\theta LL(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rust model (optimal bus engine replacement)\n",
    "\n",
    "An agent is handling a fleet of buses. Given one bus, at every period, the agent can either replace the engine and incur a cost $RC$, or perform day-to-day reparations $c(x, \\theta_1)$, where $x$ is the mileage of the bus. When the mileage of a bus increase, the cost of performing day-to-day reparations increases as well, so at some point, it is cost-efficient to just replace the engine, and reinitialize the mileage.\n",
    "\n",
    "We denote the agent's utility as:\n",
    "\n",
    "\\begin{align}\n",
    "  u(x, d, \\theta) =\n",
    "    \\begin{cases}\n",
    "      -RC(\\theta) - c(0, \\theta) + \\epsilon & \\text{if $d=1$ (replace)}\\\\\n",
    "      -c(x, \\theta) + \\epsilon & \\text{if $d=0$ (not replace)}\n",
    "    \\end{cases}       \n",
    "\\end{align}\n",
    "\n",
    "Mileage $x$ of a bus is reinitialized if $d=1$, and increases if $d=0$. Thus, we can write the transition probabilities as:\n",
    "\n",
    "\\begin{align}\n",
    "p(x'|x, d, \\theta) = \n",
    "\\begin{cases}\n",
    "g(x'- 0|\\theta) &\\text{if $d=1$}\\\\\n",
    "g(x' - x|\\theta) &\\text{if $d=0$}\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_df = pd.read_csv(r\"https://raw.githubusercontent.com/AntoineChapel/pedagogical_contents/main/rust/transition_matrix.csv\").iloc[0:, 1:]\n",
    "rust_data = pd.read_csv(r\"https://raw.githubusercontent.com/AntoineChapel/pedagogical_contents/main/rust/rust_data.csv\").iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.35103</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35103</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.637445</td>\n",
       "      <td>0.011525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351030</td>\n",
       "      <td>0.648970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    7  \\\n",
       "0   0.35103  0.637445  0.011525  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1   0.00000  0.351030  0.637445  0.011525  0.000000  0.000000  0.000000  0.0   \n",
       "2   0.00000  0.000000  0.351030  0.637445  0.011525  0.000000  0.000000  0.0   \n",
       "3   0.00000  0.000000  0.000000  0.351030  0.637445  0.011525  0.000000  0.0   \n",
       "4   0.00000  0.000000  0.000000  0.000000  0.351030  0.637445  0.011525  0.0   \n",
       "..      ...       ...       ...       ...       ...       ...       ...  ...   \n",
       "85  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "86  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "87  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "88  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "89  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "\n",
       "      8    9  ...   80   81   82   83   84       85        86        87  \\\n",
       "0   0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.000000   \n",
       "1   0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.000000   \n",
       "2   0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.000000   \n",
       "3   0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.000000   \n",
       "4   0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.000000   \n",
       "..  ...  ...  ...  ...  ...  ...  ...  ...      ...       ...       ...   \n",
       "85  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.35103  0.637445  0.011525   \n",
       "86  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.351030  0.637445   \n",
       "87  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.351030   \n",
       "88  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.000000   \n",
       "89  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.00000  0.000000  0.000000   \n",
       "\n",
       "          88        89  \n",
       "0   0.000000  0.000000  \n",
       "1   0.000000  0.000000  \n",
       "2   0.000000  0.000000  \n",
       "3   0.000000  0.000000  \n",
       "4   0.000000  0.000000  \n",
       "..       ...       ...  \n",
       "85  0.000000  0.000000  \n",
       "86  0.011525  0.000000  \n",
       "87  0.637445  0.011525  \n",
       "88  0.351030  0.648970  \n",
       "89  0.000000  1.000000  \n",
       "\n",
       "[90 rows x 90 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are ways to estimate it from the data, but to keep things accessible\n",
    "#I am giving it here: \n",
    "\n",
    "#How to interpret: every row/column index corresponds to a mileage category. 0: [0 5000]\n",
    "#                                                                            1: [5000 10000], 2: [10000 15000]...\n",
    "\n",
    "#If you are at mileage category 1, you have 35% chances to stay in that category, 64% chances to move above by one category,\n",
    "# and 1% chances to move up by 2 categories. You can guess how these can be estimated from the data.\n",
    "\n",
    "P_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8255</th>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8257</th>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8260 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      state  decision\n",
       "0       0.0       0.0\n",
       "1       0.0       0.0\n",
       "2       1.0       0.0\n",
       "3       2.0       0.0\n",
       "4       3.0       0.0\n",
       "...     ...       ...\n",
       "8255   68.0       0.0\n",
       "8256   68.0       0.0\n",
       "8257   69.0       0.0\n",
       "8258   69.0       0.0\n",
       "8259   69.0       0.0\n",
       "\n",
       "[8260 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rust_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters used in the original paper\n",
    "T = 90\n",
    "β = 0.9999\n",
    "scale=1e-3\n",
    "γ = np.euler_gamma\n",
    "x = np.arange(T)\n",
    "data = rust_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#θ is a vector of two parameters that we wish to estimate\n",
    "\n",
    "def u(x, d, θ):\n",
    "    if d==1: #replace\n",
    "        uval = -θ[1] - (scale*x*θ[0])\n",
    "    elif d==0: #not replace\n",
    "        uval = -(scale*x*θ[0])\n",
    "    return uval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational trick to avoid overflow issues:\n",
    "\n",
    "\\begin{align}\n",
    "EV_{new} &= \\log(e^{rep} + e^{wait}) \\\\\n",
    "EV_{new} &= \\log(e^{rep} + e^{wait}) - \\log(e^{EV}) + EV \\\\\n",
    "EV_{new} &= \\log(e^{rep - EV} + e^{wait - EV}) + EV\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_EV(θ, tol=1e-3, maxiter=300000, verbose=False):\n",
    "    #fixed-point algorithm\n",
    "    n_iter=0\n",
    "    EV = np.zeros((T, 1))\n",
    "    error = 1e5\n",
    "    \n",
    "    while error > tol or n_iter > maxiter:\n",
    "        EV_new = np.empty((T, 1))\n",
    "        \n",
    "        u_not_replace = u(x, 0, θ).reshape(T, 1) + β*EV\n",
    "        u_replace = u(x[0], 1, θ) + β*EV[0]\n",
    "        \n",
    "        EV_new = P@(np.log(np.exp(u_replace - EV) + np.exp(u_not_replace - EV)) + EV)\n",
    "        error = np.max(np.abs(EV - EV_new))\n",
    "        \n",
    "        n_iter +=1\n",
    "        if verbose==True:\n",
    "            print(f'Iteration {n_iter}, error: {error}')\n",
    "        EV = EV_new.copy()\n",
    "    return EV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ccp(θ):\n",
    "    EV = return_EV(θ)\n",
    "    \n",
    "    state_ccp_map = np.empty((T, 2))\n",
    "    for state in x:\n",
    "        u_not_replace = u(state, 0, θ) + β*(P[state, :] @ EV)\n",
    "        u_replace = u(0, 1, θ) + β*EV[0]\n",
    "        \n",
    "        proba_not_replace = (1/(1 + np.exp(u_replace - u_not_replace)))[0]\n",
    "        proba_replace = (1/(1 + np.exp(u_not_replace - u_replace)))[0]\n",
    "        \n",
    "        state_ccp_map[state, :] = np.array([proba_not_replace, proba_replace])    \n",
    "        \n",
    "    return state_ccp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll(θ):\n",
    "    CCP = return_ccp(θ)\n",
    "    logL = 0    \n",
    "    for s, d in data:\n",
    "        if int(d)==0:\n",
    "            logL += np.log(CCP[int(s)][0])\n",
    "        elif int(d)==1:\n",
    "            logL += np.log(CCP[int(s)][1])\n",
    "    return -logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we start close to the optimal solution to speed things up, but feel free to try alternate starting values\n",
    "theta_star = minimize(ll, x0=np.array([2, 10])).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.61826256, 10.03945503])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_star"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
