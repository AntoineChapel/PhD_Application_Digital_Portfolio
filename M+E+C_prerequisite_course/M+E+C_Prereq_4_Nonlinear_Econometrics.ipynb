{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Lecture 4: Maximum likelihood and logit</center>\n",
    "## <center>Antoine Chapel (Sciences Po & PSE) </center>\n",
    "## <center>Alfred Galichon's [math+econ+code](https://www.math-econ-code.org/) prerequisite class on numerical optimization and econometrics, in Python </center>\n",
    "Class content by Antoine Chapel. Past and present support from Alfred Galichon's ERC grant CoG-866274 is acknowledged, as well as inputs from contributors listed [here](https://www.math-econ-code.org/team). If you reuse material from this class, please cite as:\n",
    "\n",
    "Antoine Chapel, 'math+econ+code' prerequisite class on numerical optimization and econometrics, January 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "* Discrete Choice Methods with Simulation 2nd Ed, (Train, K) 2009\n",
    "* Microeconometrics Methods and Applications, (Cameron, Trivedi), 2005\n",
    "* Alfred Galichon, 'math+econ+code' masterclass on optimal transport and economic applications, January 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Outline\n",
    "\n",
    "The Math+Econ+Code January masterclass is divided into an optimization-oriented part and an econometrics-oriented part. To understand well how to apply Optimal Transport to econometrics, some econometrics building blocks are absolutely essential, especially Logit theory. We will carefully study this topic to prepare you best.\n",
    "\n",
    "First, we will discuss a bit of Maximum Likelihood to remind the basics of nonlinear econometrics. In particular, we will consider Poisson regression. Then, we will rederive step-by-step the logit estimation technique.\n",
    "\n",
    "Across this lecture, we will make use of numerical examples in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood:\n",
    "Econometrics start with a sample and some distributional assumption.\n",
    "If you have the following sample: [81, 75, 100, 92, 64], what is the chance that it has been generated by a normal distribution of mean 10: $\\mathcal{N}(10, 1)$ ? The chance is very low, actually close to 0. The chance/the **likelihood** that it has been generated by $\\mathcal{N}(82, 1)$ is much higher. Maximum likelihood allows us to formalize this idea through an optimization process.\n",
    "\n",
    "\n",
    "Let us start by finding a normal estimator through Maximum Likelihood, as a first example.\n",
    "\n",
    "\\begin{align}\n",
    "f(y | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{ (y - \\mu)^2 }{2\\sigma^2}}\n",
    "\\end{align}\n",
    "\n",
    "The likelihood function is equal to the joint density of the distribution:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} (\\mu, \\sigma^2 | y) = \\prod_{i=1}^N  \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{ (y - \\mu)^2 }{2\\sigma^2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathcal{L} (\\mu, \\sigma^2 | y) = \\Bigg(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\Bigg)^n e^{{-\\frac{1}{2 \\sigma^2}} \\sum_{i=1}^n (y_i - \\mu)^2 }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\log \\mathcal{L} = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    " = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2)-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i^2 - 2y_i\\mu + \\mu^2)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we find the most likely value for $\\mu$ by taking FOC.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log \\mathcal{L}}{\\partial \\mu} = -\\frac{1}{2 \\sigma^2} 2 \\sum (\\mu - y_i) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (y_i - \\mu) \\hspace{2pt} \\bigg|_{\\hat{\\mu}} = 0\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\hat{\\mu} = \\frac{1}{n} \\sum_{i} y_i\n",
    "\\end{align}\n",
    "\n",
    "Exercise 1: follow the same procedure for variance estimator $\\hat{\\sigma}^2$\n",
    "\n",
    "Exercise 2: find the OLS estimator (for simple regression) through maximum likelihood. Hint: the distributional assumption is that $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, so $y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_1, \\sigma^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Regression\n",
    "\n",
    "Here, let us start from the assumption that data is distributed according to the Poisson distribution. This allows us to do Poisson Regression.\n",
    "\n",
    "PMF of the Poisson distribution: $f(y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^y}{y!}$\n",
    "$E[y] = Var[y] = \\lambda$ \n",
    "\n",
    "First, what is Poisson regression, and why should we even bother with anything more complicated than OLS ?\n",
    "\n",
    "When you do OLS, you rely on the assumption that your the endogenous variable is normally distributed. This works fine most of the time, but sometimes this assumption is not realistic. Poisson regression is used to model situations where the endogenous variable is $(i)$ discrete and $(ii)$ positive. The typical example is the number of visits to the doctor that an individual makes per year. A lot of people only go once a year, a bit less go twice a year, very few go more than 6 times a year. Nobody goes -2 times a year. Therefore this process is better approximated by a Poisson distribution than a normal distribution. Although OLS would give some results, the Poisson assumption will give better ones.\n",
    "\n",
    "An OLS consists in saying that $E[Y|X] = X'\\beta$.\n",
    "\n",
    "**Generalized Linear Models**, of which Poisson regression is a very often used subtype, assumes instead that:\n",
    "\n",
    "\\begin{align}\n",
    "g(E[Y|X]) &= X'\\beta \\\\\n",
    "E[Y|X] &= g^{-1}(X'\\beta)\n",
    "\\end{align}\n",
    "\n",
    "Where $g$ is called the **link** function. In OLS, which is also a subtype of GLM, the link function is the identity: $g(x) = x$. In Poisson regression, the link function is the natural logarithm. So, we have:\n",
    "\n",
    "\\begin{align}\n",
    "E[Y|X] = e^{X'\\beta}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go back to the Poisson distribution. The density of the Poisson regression model for one observation is as follows:\n",
    "\\begin{align}\n",
    "f(y_i|x_i, \\beta) = \\frac{e^{-\\exp(x_i' \\beta)} \\exp(x_i' \\beta)^{y_i}}{y_i!}\n",
    "\\end{align}\n",
    "\n",
    "If the observations are i.i.d, we can derive the joint density of a sample of size $N$ as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\prod_{i=1}^N \\frac{e^{-\\exp(x_i' \\beta)} \\exp(x_i' \\beta)^{y_i}}{y_i!}\n",
    "\\end{align}\n",
    "\n",
    "This is the **likelihood** ($\\mathcal{L}$) function, on which we want to maximize for the optimal parameter (here, $\\beta$). Since a sum is always much easier to take derivative on than products, it is better to take the **log-likelihood**:\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N \\log\\Big(\\frac{e^{-\\exp(x_i' \\beta)} \\exp(x_i' \\beta)^{y_i}}{y_i!}\\Big) = \\frac{1}{N} \\sum_{i=1}^N -\\exp(x_i' \\beta) + y_i x_i' \\beta - \\log y_i !\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log\\mathcal{L}}{\\partial \\beta} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\exp(x_i'\\beta))x_i' = 0\n",
    "\\end{align}\n",
    "\n",
    "Given that this equation has no closed form solution, we need to rely on numerical optimization methods, such as the ones provided by Scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "Poisson_reg = linear_model.PoissonRegressor()\n",
    "X_val = np.array([[1, 2], [2, 3], [3, 5], [4, 3]])\n",
    "y_val = np.array([12, 17, 22, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716538991248989"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Poisson_reg.fit(X_val, y_val)\n",
    "Poisson_reg.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13488995, 0.09463608])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Poisson_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIY approach: let's recycle our lecture on numerical optimisation: we want to find $\\hat{\\beta} = \\arg \\max_\\beta \\log \\mathcal{L}(X, y; \\beta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 3., 5.],\n",
       "       [1., 4., 3.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we are doing things by hand, let's add a column of ones to account for a constant\n",
    "X_val = np.concatenate((np.ones((X_val.shape[0], 1)), X_val), axis=1)\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logL_Poisson(β, X, y):\n",
    "    sum_logl = 0\n",
    "    for i, x_i in enumerate(X):\n",
    "        y_i = y[i]\n",
    "        sum_logl += -np.exp(x_i @ β) + y_i*(x_i @ β) #- np.log(float(factorial(y_i)))\n",
    "    nobs = X.shape[0]  \n",
    "    return -(sum_logl/nobs)\n",
    "\n",
    "#Why '-' ? The scipy optimisation function is written explicitly for minimization. Therefore,\n",
    "#we will minimize (-likelihood), which is equivalent as maximising the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 2.3679256828901867\n",
       " hess_inv: array([[ 0.69516226, -0.08215793, -0.12018918],\n",
       "       [-0.08215793,  0.0582273 , -0.02220999],\n",
       "       [-0.12018918, -0.02220999,  0.05237436]])\n",
       "      jac: array([5.36441803e-07, 1.25169754e-06, 2.68220901e-06])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 70\n",
       "      nit: 10\n",
       "     njev: 14\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([2.19900236, 0.14059106, 0.09672536])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Option 1: half-DIY\n",
    "minimize(logL_Poisson, x0 = np.array([0, 0, 0]), args=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We nearly get the same coefficients as the sklearn function (the first value is a constant, and the two next, 0.14 and 0.09 are the weights for our two variables)\n",
    "\n",
    "Overall, we can model $\\hat{y} = e^{2.2 + 0.14 X_1 + 0.09 X_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's adapt a bit our functions from yesterday to fit them to maximum likelihood\n",
    "#Option 2: full-DIY\n",
    "\n",
    "def num_gradient(f, x, X, y):\n",
    "    partial_derivatives_vector=np.empty(x.shape[0])\n",
    "    for index in range(x.shape[0]):\n",
    "        e_ind = np.zeros(x.shape[0])\n",
    "        e_ind[index] = 1.0\n",
    "        h = 1e-5\n",
    "        partial_derivatives_vector[index] = (f(x + e_ind*h, X, y) - f(x - e_ind*h, X, y))/(2*h)\n",
    "    return partial_derivatives_vector\n",
    "\n",
    "def num_hessian(f, x, X, y):\n",
    "    hessian = np.empty((x.shape[0], x.shape[0]))\n",
    "    for index in range(x.shape[0]):\n",
    "          for jindex in range(x.shape[0]):\n",
    "                e_ind = np.zeros(x.shape[0])\n",
    "                e_ind[jindex] = 1.0\n",
    "                h = 1e-5\n",
    "                hessian[index, jindex] = (num_gradient(f, x + e_ind*h, X, y)[index] - num_gradient(f, x - e_ind*h, X, y)[index])/(2*h)\n",
    "    return hessian\n",
    "\n",
    "def gradient_descent(f, x_1, X, y, step=1, tol=1e-5, verbose=False):\n",
    "    \"\"\"Implements Gradient Descent using the Newton-Raphson algorithm. Gradients and Hessians are computed numerically.\\n\n",
    "    x_1: starting point of the iterative process\\n\n",
    "    X: Explanatory variables\\n\n",
    "    y: Dependent variable\"\"\"\n",
    "    \n",
    "    \n",
    "    error = 1e6\n",
    "    x_hist = []\n",
    "    maxiter=100\n",
    "    \n",
    "    x = x_1\n",
    "    n_iter=1\n",
    "    \n",
    "    while error > tol:\n",
    "        g_k = num_gradient(f, x, X, y)\n",
    "        if verbose==True:\n",
    "            print(f'iteration: {n_iter}, θ_hat: {np.round(x, 3)}, gradient: {np.round(g_k, 3)}, likelihood: {np.round(f(x, X,y), 3)}')\n",
    "        \n",
    "        H_k = num_hessian(f, x, X, y)\n",
    "        try:\n",
    "            A_k = -np.linalg.inv(H_k)\n",
    "        except:\n",
    "            A_k = np.random.normal(0, 1, size=x.shape[0])*np.identity(x.shape[0])\n",
    "            print(\"ERROR: Failure to invert the Hessian\")\n",
    "            \n",
    "        x = x + step*A_k@g_k\n",
    "        x_hist.append(x)\n",
    "        error = max(abs(g_k))\n",
    "        n_iter+=1\n",
    "    return x, x_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, θ_hat: [0. 0. 0.], gradient: [-17.   -46.5  -58.75], likelihood: 37.836\n",
      "iteration: 2, θ_hat: [5.572 2.343 1.714], gradient: [5.24665615e+08 1.70488523e+09 2.35652622e+09], likelihood: 524665348.706\n",
      "iteration: 3, θ_hat: [4.572 2.343 1.714], gradient: [1.93013663e+08 6.27192079e+08 8.66917633e+08], likelihood: 193013414.686\n",
      "iteration: 4, θ_hat: [3.57  2.343 1.714], gradient: [7.10057051e+07 2.30731052e+08 3.18921176e+08], likelihood: 71005474.532\n",
      "iteration: 5, θ_hat: [2.59  2.34  1.713], gradient: [2.61217057e+07 8.48812038e+07 1.17324535e+08], likelihood: 26121493.145\n",
      "iteration: 6, θ_hat: [1.604 2.337 1.711], gradient: [ 9609675.502 31226033.824 43161269.932], likelihood: 9609480.857\n",
      "iteration: 7, θ_hat: [0.623 2.333 1.709], gradient: [ 3535219.42  11487404.75  15878130.722], likelihood: 3535042.707\n",
      "iteration: 8, θ_hat: [-0.38   2.334  1.71 ], gradient: [1300528.574 4225975.42  5841220.758], likelihood: 1300369.872\n",
      "iteration: 9, θ_hat: [-1.377  2.334  1.71 ], gradient: [ 478432.115 1554636.813 2148848.569], likelihood: 478291.404\n",
      "iteration: 10, θ_hat: [-2.35   2.329  1.707], gradient: [176001.069 571906.563 790501.257], likelihood: 175878.263\n",
      "iteration: 11, θ_hat: [-3.298  2.319  1.702], gradient: [ 64742.331 210379.967 290792.872], likelihood: 64637.346\n",
      "iteration: 12, θ_hat: [-4.152  2.293  1.689], gradient: [ 23812.627  77381.759 106960.538], likelihood: 23725.134\n",
      "iteration: 13, θ_hat: [-4.749  2.22   1.652], gradient: [ 8755.688 28454.738 39332.997], likelihood: 8684.797\n",
      "iteration: 14, θ_hat: [-4.809  2.05   1.566], gradient: [ 3216.96  10456.284 14455.411], likelihood: 3160.804\n",
      "iteration: 15, θ_hat: [-4.025  1.727  1.403], gradient: [1180.154 3836.498 5305.706], likelihood: 1135.791\n",
      "iteration: 16, θ_hat: [-2.615  1.295  1.182], gradient: [ 431.599 1402.532 1941.766], likelihood: 396.782\n",
      "iteration: 17, θ_hat: [-1.181  0.868  0.954], gradient: [156.359 507.213 704.426], likelihood: 130.778\n",
      "iteration: 18, θ_hat: [0.049 0.503 0.735], gradient: [ 55.044 177.668 248.884], likelihood: 38.779\n",
      "iteration: 19, θ_hat: [1.03  0.243 0.517], gradient: [17.864 56.935 81.587], likelihood: 10.199\n",
      "iteration: 20, θ_hat: [1.726 0.13  0.302], gradient: [ 4.679 14.516 21.907], likelihood: 3.347\n",
      "iteration: 21, θ_hat: [2.098 0.129 0.146], gradient: [0.743 2.225 3.605], likelihood: 2.407\n",
      "iteration: 22, θ_hat: [2.194 0.14  0.099], gradient: [0.036 0.106 0.177], likelihood: 2.368\n",
      "iteration: 23, θ_hat: [2.199 0.141 0.097], gradient: [0.    0.    0.001], likelihood: 2.368\n",
      "iteration: 24, θ_hat: [2.199 0.141 0.097], gradient: [0. 0. 0.], likelihood: 2.368\n"
     ]
    }
   ],
   "source": [
    "beta_hat, beta_hist = gradient_descent(logL_Poisson, np.zeros(3), X_val, y_val, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.19900227, 0.1405911 , 0.09672535])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit\n",
    "\n",
    "Logit is a tool at the basis of discrete choice models. We live in a discrete world: when you choose a level of insurance or an airplane ticket, you choose between basic, premium, elite, or whatever the name is. If you want to go from Paris to London, you need to choose between airplane, train, boat, car or swimming. What drives your choice ? Probably comfort, speed, and price, among others.\n",
    "\n",
    "As you may know already, OLS is unfit for modelling this type of process. Let us say that you can only choose between plane ($y_i = 1$) or train ($y_i = 0$). For any explanatory variable $x$, we are looking for the probability P($y_i = 1 | x$). OLS doesn't know that we are looking for a probability, so it may give probability $>$ 1 or $< 0$, which doesn't make sense. For this reason the logit framework is more appropriated than OLS.\n",
    "\n",
    "Logit relies on an important concept which is unobserved heterogeneity: it can be understood as the \"things\" that make a given alternative more or less desirable, but that we cannot observe it in the data. For example, if you study the market for chairs, it may be easy to observe the material used, but much less easy to observe the level of comfort.\n",
    "\n",
    "For logit, the distributional assumption of this unobserved heterogeneity is that it is distributed following the Gumbel distribution. It is also called Extreme Value Type 1.\n",
    "\n",
    "The CDF of this distribution is the following: $F(\\epsilon) = e^{-e^{-\\epsilon}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee4b387b08>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.random.gumbel(size=100000)\n",
    "sns.kdeplot(X, label=\"Standard Gumbel Distribution\", bw_adjust=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25a15493e08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = np.random.gumbel(size=100000)\n",
    "Z = X-Y\n",
    "\n",
    "sns.kdeplot(Z, label=\"Logistic Distribution\", bw_adjust=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logit\n",
    "\n",
    "An agent $i$ chooses among alternatives $j \\in \\mathcal{J}$. The utility derived by this agent from choosing alternative $j$ is written as follows:\n",
    "\n",
    "\\begin{align}\n",
    "U_{ij} = V_{ij} + \\epsilon_{ij}, \\hspace{5pt} \\forall j\n",
    "\\end{align}\n",
    "\n",
    "Where V is deterministic, a function of product characteristics (like $V$ = $X'\\beta$), and $\\epsilon$ is the unobserved heterogeneity.\n",
    "\n",
    "The probability that this agent chooses $j$ rather than $k$ given $\\epsilon_{ij}$ is thus:\n",
    "\n",
    "\\begin{align}\n",
    "P(U_{ik} < U_{ij} | \\epsilon_{ij} \\hspace{5pt} \\forall k \\neq j) \\\\\n",
    "= P(V_{ik} + \\epsilon_{ik} < V_{ij} + \\epsilon_{ij} \\hspace{5pt} \\forall k \\neq j) \\\\\n",
    "= P(\\epsilon_{ik} < \\epsilon_{ij} + V_{ij} - V_{ik} \\hspace{5pt} \\forall k \\neq j)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    " = F(\\epsilon_{ij} + V_{ij} - V_{ik} \\hspace{5pt} \\forall k \\neq j) = \\prod_{k \\neq j}e^{-e^{-{\\epsilon_{ij} + V_{ij} - V_{ik}}}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step are very algebraic, and you only need to know and be able to use the result. But it is good to rederive the results once in your life. \n",
    "Since we do not know $\\epsilon_{ij}$ so we need to integrate over possible values.\n",
    "\n",
    "\\begin{align}\n",
    "P_{ij} = \\int_{-\\infty}^{+\\infty} \\prod_{k \\neq j} e^{-e^{-{\\epsilon_{ij} + V_{ij} - V_{ik}}}} f(\\epsilon_{ij}) d \\epsilon_{ij}\n",
    "\\end{align}\n",
    "We know that: \n",
    "\\begin{align}\n",
    "f(\\epsilon_{ij}) = e^{-\\epsilon} e^{-e^{-\\epsilon}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P_{ij} = \\int_{-\\infty}^{+\\infty} \\Bigg(\\prod_{k} e^{-e^{-{\\epsilon_{ij} + V_{ij} - V_{ik}}}}\\Bigg) e^{-\\epsilon_{ij}} d \\epsilon_{ij} \\\\\n",
    "= \\int_{-\\infty}^{+\\infty} exp\\Bigg( \\sum_k e^{-(\\epsilon_{ij} + V_{ij} - V_{ik})} \\Bigg) e^{-\\epsilon_{ij}} d\\epsilon_{ij} \\\\\n",
    "= \\int_{-\\infty}^{+\\infty} exp\\Bigg( -e^{-\\epsilon_{ij}} \\sum_k e^{-(V_{ij} - V_{ik})} \\Bigg) e^{-\\epsilon_{ij}} d\\epsilon_{ij} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notational trick: we define $t = e^{-\\epsilon_{ij}}$: $\\hspace{5pt}$ $\\frac{dt}{d\\epsilon_{ij}} = -e^{-\\epsilon_{ij}}$, so $dt = -e^{-\\epsilon_{ij}}d\\epsilon_{ij}$\n",
    "\n",
    "We rewrite this integral as being integrated over $dt$, noting that, when $\\epsilon$ tended towards $+\\infty$, t would tend to 0, and when $\\epsilon$ tended towards $-\\infty$, $t$ tended towards $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "P_{ij} = \\int_{+\\infty}^0 exp \\Bigg( -t \\sum_k e^{-(V_{ij} - V{ik})} \\Bigg) (-dt) \\\\\n",
    "=  \\int_{0}^{+\\infty} exp \\Bigg( -t \\sum_k e^{-(V_{ij} - V_{ik})} \\Bigg) dt\n",
    "\\end{align}\n",
    "\n",
    "This expression is then relatively easy to integrate since it's based on an exponential:\n",
    "\n",
    "\\begin{align}\n",
    "= \\Bigg[ \\frac{exp \\Big( -t \\sum_k e^{-(V_{ij} - V_{ik})} \\Big)}{-\\sum_k e^{-(V_{ij} - V_{ik})} } \\Bigg]_0^{+\\infty} \\\\\n",
    "= \\frac{1}{\\sum_k e^{-(V_{ij} - V_{ik})}} \\\\\n",
    "= \\frac{e^{V_{ij}}}{\\sum_k e^{V_{ik}}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This really succint closed-form expression, which is allowed by the distributional assumption made on the vector of $\\epsilon$, allows us to transform observable characteristics into choice probabilities. Based on \"what we can observe\" about alternative $j$, we can express the probability that an individual will choose it. How to go now from this to a regression allowing us to estimate parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us normalize the utility of alternative $j=0$ to $V_0 = 0$, and forget about the individual subscript for now. We can do so by interpreting $P_{j}$ as the \"market share\" of alternative $j$, which we denote $s_j$. These market shares are usually easily observed from the data. We will typically make the assumption that $V_j$ is linear in characteristics of $j$: $V_j = X_j'\\theta + \\xi_j$, where $\\xi_j$ is a normally distributed error term. The question is now, how can we estimate $\\theta$ ?\n",
    "\n",
    "\\begin{align}\n",
    "s_j = \\frac{e^{V_j}}{\\sum_k e^{V_k}} \\\\\n",
    "s_0 = \\frac{e^{V_0}}{\\sum_k e^{V_k}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{s_j}{s_0} &= \\frac{e^{V_j}}{e^{V_0}} = \\frac{e^{V_j}}{e^0} = e^{V_j} \\\\\n",
    "\\log(s_j) - \\log(s_0) &= V_j \\\\\n",
    "\\log(s_j) - \\log(s_0) &= X_j'\\theta + \\xi_j\n",
    "\\end{align}\n",
    "\n",
    "This last expression is just an OLS regression equation, that you can estimate for every alternative $j$. This model is relatively simple. In the field of empirical Industrial Organization, more subtle methods have been employed, one of which we will study tomorrow and in the masterclass (Berry Levinsohn Pakes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: in IO (industrial organization), the price of alternative $j$ is often considered as one of the characteristics of alternative $j$ employed in the regression. On the left-hand-side of the regression equation: a product's market share, and on the r.h.s, the product's price. What issue can arise there ? How would you solve it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logit as Poisson regression\n",
    "\n",
    "The unexpected connection, that will come back in the masterclass. We make a very simple introduction to the topic using the same dataset as the one used in the masterclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "thepath = 'https://raw.githubusercontent.com/math-econ-code/mec_optim_2021-01/master/data_mec_optim/demand_travelmode/'\n",
    "df =  pd.read_csv(thepath+'travelmodedata.csv')\n",
    "df['choice'] = df['choice'].map({\"no\": 0, \"yes\": 1}).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I: nb of individuals (210)\n",
    "I = max(df['individual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#J: number of alternatives (4)\n",
    "J = len(set(df['mode']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K: nb of characteristics (3: travel, income, gcost)\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = df[['travel', 'income', 'gcost']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three variables of interest are \"travel, -(travel*income), -gcost\". We create an array $X_{ij}$ that contains these. Then, we normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.28107292,  0.86778229,  0.85204596],\n",
       "       [-0.37873479,  0.25143482,  0.83120323],\n",
       "       [-0.22945091,  0.14946557,  0.85204596],\n",
       "       ...,\n",
       "       [ 0.60985625, -1.94203704, -0.94042907],\n",
       "       [ 0.58995173, -1.91484524, -0.48188895],\n",
       "       [ 0.1785917 , -1.35288138,  0.35182037]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij = np.empty(covariates.shape)\n",
    "X_ij[:, 0] = covariates[:, 0]\n",
    "X_ij[:, 1] = -(covariates[:, 0]*covariates[:, 1])\n",
    "X_ij[:, 2] = -(covariates[:, 2])\n",
    "\n",
    "X_mean = X_ij.mean(axis=0)\n",
    "X_std = X_ij.std(axis=0, ddof=1) #(ddof=1 is an option that computes the unbiased estimator of variance)\n",
    "\n",
    "X_ij_n = (X_ij - X_mean)/X_std\n",
    "X_ij_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this array contain ? $(4*210) \\times 3$ observations, corresponding to the perception of 210 individuals of 4 means of transport, along three variables. Below, we get a vector of size $210$ that contains the eventual choice of every individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_data = df['choice'].to_numpy().reshape(I, J)\n",
    "choice_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that representative utility is indexed by individual, we can no more rely on the formulas used earlier, at least not immediately. Check that you see why. So, let's derive multinomial logit like a maximum likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "P_{ij}|\\theta &= \\frac{e^{X_{ij}'\\theta}}{\\sum_k e^{X_{ik}'\\theta}}\\\\\n",
    "\\mathcal{L}(\\theta) &= \\prod_{i=1}^I \\Big(\\frac{e^{X_{ij}'\\theta}}{\\sum_k e^{X_{ik}'\\theta}} \\Big)\\\\\n",
    "\\log \\mathcal{L}(\\theta) &= \\frac{1}{N} \\sum_{i=1}^N \\log \\Big(P_{ij}|\\theta \\Big)\n",
    "\\end{align}\n",
    "\n",
    "Note that $\\frac{1}{N}$ is added to avoid the sum to go to $+\\infty$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta} = \\arg \\max_\\theta \\log \\mathcal{L}(\\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us put this in word: for every individual $i$ , we know $X_{ij}$ and the eventual choice, which we denote $\\mu_{ij}$. Given a candidate parameter $\\theta$, we can predict a choice probability $P_{ij} | \\theta$, which is the **likelihood** of $i$ having this probability to make the observed choice. To find the right set of parameters, we want to maximize this likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard multinomial logit\n",
    "def log_l_logit(θ):\n",
    "    logl = 0\n",
    "    U = (X_ij_n @ θ).reshape(I, J) #entry (i, j) contains the utility of individual i to choose alternative j\n",
    "    \n",
    "    for i in range(I):\n",
    "        choice_index = np.where(choice_data[i, :] == 1)\n",
    "        U_i = U[i, :]\n",
    "        P_ij = np.exp(U_i[choice_index])/(np.sum(np.exp(U_i)))\n",
    "        \n",
    "        logl = logl + np.log(P_ij)\n",
    "    \n",
    "    #we return -logl rather than logl because the standard optim package does minimization\n",
    "    return -logl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18624277, 0.46897839, 0.5505769 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ_hat = minimize(log_l_logit, x0=np.zeros(K)).x\n",
    "θ_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note the following:\n",
    "This is the expression for logit log-likelihood.\n",
    "\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(\\theta) &= \\frac{1}{N} \\sum_{i=1}^N \\log \\Big(\\frac{e^{x_{ij}'\\theta}}{\\sum_k e^{x_{ik}'\\theta}}\\Big) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rewrite it by summing over alternatives. This is actually a way to write that is more formally correct. Let us denote $\\mu_{ij}$ the choice of individual $i$, where $\\mu_{ij} = 1$ if $i$ chose $j$, and $\\mu_{ij} = 0$ if $i$ didn't choose $j$.\n",
    "\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(\\theta) &= \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^J \\mu_{ij} \\log \\Big(\\frac{e^{x_{ij}'\\theta}}{\\sum_k e^{x_{ik}'\\theta}}\\Big) \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^J \\Big( \\mu_{ij} \\exp({x_{ij}'\\theta}) - \\mu_{ij}\\log \\sum_k e^{x_{ik}'\\theta}\\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us come back to Poisson regression: this was our initial expression for Poisson likelihood.\n",
    "\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\log\\Big(\\frac{e^{-\\exp(x_i' \\theta)} \\exp(x_i' \\theta)^{y_i}}{y_i!}\\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us rewrite it with individual fixed effect $\\phi_i$. Also, the $y_i$ was our endogenous variable in the simple case. here the endogenous variable is $\\mu_i$, and we would like to predict $\\mu_{ij}$. So let us add a sum over alternatives: \n",
    "\n",
    "\\begin{align}\n",
    "\\log \\mathcal{L}(\\theta) &= \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^J \\log\\Big(\\frac{e^{-\\exp( \\phi_i + x_{ij}' \\theta)} \\exp(\\phi_i + x_{ij}'\\theta)^{\\mu_{ij}}}{\\mu_{ij}!}\\Big) \\\\\n",
    " &= \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^J \\Big(-\\exp(\\phi_i + x_{ij}'\\theta) + \\mu_{ij}(\\phi_i + x_{ij}'\\theta) - log(\\mu_{ij}!) \\Big) \\\\\n",
    " &= \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^J \\Big(-\\exp(\\phi_i + x_{ij}'\\theta) + \\mu_{ij}(\\phi_i + x_{ij}'\\theta) \\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see, maximizing this expression yields the same estimator as doing maximum likelihood in the logit case. One last technicality: how to add fixed effects (fe) to a Poisson Regression ? Think of an individual fe as an individual-specific intercept. If we want to add a constant for everyone, we just add a vector of ones to the data, like we did with the Poisson regression earlier. Now, we are separating between every individual, so instead of a vector, we add a matrix that would look like this for 3 individuals, 2 alternatives:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\ \n",
    "1 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Such matrices are built through the kronecker product $\\otimes$. If you are not familiar with this, check that you [understand it](https://en.wikipedia.org/wiki/Kronecker_product) before attending the masterclass. The matrix above could be build the following way:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{I}_3 \\otimes \\mathcal{1}_2 = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{bmatrix} \\otimes \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\ \n",
    "1 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_matrix = np.kron(np.identity(I), np.ones((J, 1)))\n",
    "fe_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.281073</td>\n",
       "      <td>0.867782</td>\n",
       "      <td>0.852046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.378735</td>\n",
       "      <td>0.251435</td>\n",
       "      <td>0.831203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.229451</td>\n",
       "      <td>0.149466</td>\n",
       "      <td>0.852046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.015679</td>\n",
       "      <td>0.686504</td>\n",
       "      <td>1.685755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.387230</td>\n",
       "      <td>0.962306</td>\n",
       "      <td>0.893731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0.079069</td>\n",
       "      <td>0.434008</td>\n",
       "      <td>0.601933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>-1.148376</td>\n",
       "      <td>0.459905</td>\n",
       "      <td>0.497720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0.609856</td>\n",
       "      <td>-1.942037</td>\n",
       "      <td>-0.940429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.589952</td>\n",
       "      <td>-1.914845</td>\n",
       "      <td>-0.481889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0.178592</td>\n",
       "      <td>-1.352881</td>\n",
       "      <td>0.351820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2    3    4    5    6    7    8    9    ...  \\\n",
       "0   -1.281073  0.867782  0.852046  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1   -0.378735  0.251435  0.831203  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2   -0.229451  0.149466  0.852046  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3   -1.015679  0.686504  1.685755  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4   -1.387230  0.962306  0.893731  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "..        ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "835  0.079069  0.434008  0.601933  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "836 -1.148376  0.459905  0.497720  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "837  0.609856 -1.942037 -0.940429  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "838  0.589952 -1.914845 -0.481889  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "839  0.178592 -1.352881  0.351820  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "     203  204  205  206  207  208  209  210  211  212  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "835  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "836  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "837  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "838  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "839  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[840 rows x 213 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij_fe = np.concatenate((X_ij_n, fe_matrix), axis=1)\n",
    "\n",
    "#Our data is now ready for Poisson regression with fixed effects:\n",
    "pd.DataFrame(X_ij_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "μ_vec = df['choice'].to_numpy()\n",
    "#due to the fe, our gradient descent function risks taking ages to find a solution because the Newton-Raphson algorithm\n",
    "#requires computing the Hessian at every iteration. Since it would take too long, we rely on the one provided by sklearn\n",
    "\n",
    "\n",
    "#This may take a while. For a (much) faster implementation, try to use the PoissonRegressor provided by sklearn.\n",
    "θ_hat_poisson = minimize(logL_Poisson, x0 = np.random.normal(0, 1, size=213), args=(X_ij_fe, μ_vec)).x[:K]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Poisson Regression tool provided by sklearn [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ_hat obtained through logit: [0.18624277 0.46897839 0.5505769 ]\n",
      "θ_hat obtained through Poisson: [0.18817743 0.47171693 0.5496784 ]\n"
     ]
    }
   ],
   "source": [
    "print(f'θ_hat obtained through logit: {θ_hat}')\n",
    "print(f'θ_hat obtained through Poisson: {θ_hat_poisson}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
